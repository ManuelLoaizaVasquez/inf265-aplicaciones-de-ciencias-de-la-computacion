{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFgl_WT_msoB"
   },
   "source": [
    "# Aplicaciones de Ciencias de la Computación (INF265)\n",
    "## Laboratorio 1: Agente aspirador de 6 posiciones\n",
    "\n",
    "Al finalizar el presente laboratorio se debe tener implementado el entorno de trabajo del agente aspirador de 6 posiciones y un programa reflexivo simple para dicho agente. Las posiciones del entorno son denotadas como loc_A, loc_B, loc_C, loc_D, loc_E y loc_F. Cada una de estas posiciones puede tener el estado 'Dirty' o 'Clean'.  \n",
    "Al final del notebook deberas responder a las preguntas planteadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLu9fqhim_KE"
   },
   "source": [
    "# Clase <b>Thing</b>\n",
    "\n",
    "Esta clase generica representa cualquier objeto fisico que puede aparecer en un <b>Ambiente</b>. (No editar)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FkfexJt4nEUU"
   },
   "outputs": [],
   "source": [
    "class Thing(object):\n",
    "\n",
    "    def is_alive(self):\n",
    "        \"\"\"Cosas 'vivas'deben retornar true.\"\"\"\n",
    "        return hasattr(self, 'alive') and self.alive\n",
    "\n",
    "    def show_state(self):\n",
    "        \"\"\"Muestra el estado interno del agente. Subclases deben sobreescribir esto.\"\"\"\n",
    "        print(\"I don't know how to show_state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8cWKKzLnGtD"
   },
   "source": [
    "# Clase <b>Agent</b>\n",
    "\n",
    "Un agente es una subclase de Thing con un slot obligatorio: <b>.program</b>, el cual almacena la funcion que implementa el <b>programa del agente</b>. Esta funcion debe tomar como argumento la <b>percepcion</b> del agente y debe retornar una <b>accion</b>. La definicion de Percepcion y Accion depende del ambiente de trabajo (environment) donde el agente existe. El agente tambien puede tener el slot <b>.performance</b>, que mide el desempeño del agente en su ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "adL1GxO4nIOg"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "class Agent(Thing):\n",
    "\n",
    "    def __init__(self, program=None):\n",
    "        self.alive = True\n",
    "        self.performance = 0\n",
    "        assert isinstance(program, collections.Callable)\n",
    "        self.program = program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4TAAIxHnJte"
   },
   "source": [
    "# Clase <b>Environment</b>\n",
    "\n",
    "Esta clase abstracta representa un entorno de tareas. Clases de entornos reales heredan de esta. En un entorno tipicamente se necesitará implementar 2 cosas:\n",
    "<b>percept</b>, que define la percepción que el agente ve; y \n",
    "<b>execute_action</b>, que define los efectos de ejecutar una acción. \n",
    "El entorno mantiene una lista de .things y .agents (el cual es un subconjunto de .things). Cada elemento de .things tiene un slot .location. (No editar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C9uzpUSMnLjK"
   },
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.things = []\n",
    "        self.agents = []\n",
    "\n",
    "    def thing_classes(self):\n",
    "        return []  # List of classes that can go into environment\n",
    "\n",
    "    def percept(self, agent):\n",
    "        \"\"\"Retorna la percepción que el agente 'agent' ve en este punto.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        \"\"\"El agente 'agent' ejecuta una acción 'action' en el entorno.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def default_location(self, thing):\n",
    "        \"\"\"Localización por defecto para colocar una nueva cosa sin localización especificada.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"Retorna True si no hay ningón agente vivo\"\"\"\n",
    "        return not any(agent.is_alive() for agent in self.agents)\n",
    "\n",
    "    def add_thing(self, thing, location=None):\n",
    "        \"\"\"Añade una cosa thing al entorno en la localización location. \n",
    "           Si thing es un programa de agente, crea un nuevo agente con ese programa.\"\"\"\n",
    "        if not isinstance(thing, Thing):\n",
    "            thing = Agent(thing)\n",
    "        assert thing not in self.things, \"No añade la misma cosa dos veces\"\n",
    "        thing.location = location if location is not None else self.default_location(thing)\n",
    "        self.things.append(thing)\n",
    "        if isinstance(thing, Agent):\n",
    "            thing.performance = 0\n",
    "            self.agents.append(thing)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Ejecuta un paso del entorno (llama a los programas de los agentes, obtiene sus acciones y las ejecuta). \"\"\"\n",
    "        if not self.is_done():\n",
    "            actions = []\n",
    "            for agent in self.agents:\n",
    "                if agent.alive:\n",
    "                    actions.append(agent.program(self.percept(agent)))\n",
    "                else:\n",
    "                    actions.append(\"\")\n",
    "            for (agent, action) in zip(self.agents, actions):\n",
    "                self.execute_action(agent, action)\n",
    "\n",
    "    def run(self, steps=1000):\n",
    "        \"\"\"Ejecuta steps pasos en el entorno.\"\"\"\n",
    "        for step in range(steps):\n",
    "            if self.is_done():\n",
    "                return\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7olSAZHnQav"
   },
   "source": [
    "#### Clase <b>VacuumEnvironment</b>\n",
    "\n",
    "Esta clase implementa el entorno del aspirador de 6 posiciones: loc_A, loc_B, loc_C, loc_D, loc_E y loc_F. Cada una de estas posiciones puede tener el estado 'Dirty' o 'Clean'. Un agente en este entorno percibe su localizacion y el estado de la misma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "SMLJe3lPnRW8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Considere el siguiente ambiente:\n",
    "\n",
    "#############\n",
    "# A # B # C #\n",
    "#############\n",
    "# D # E # F #\n",
    "#############\n",
    "# Complete las posiciones reemplazando los guiones bajos por números\n",
    "loc_A, loc_B, loc_C, loc_D, loc_E, loc_F = (0,0), (0,1), (0, 2), (1, 0), (1, 1), (1, 2)\n",
    "\n",
    "class VacuumEnvironment(Environment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.status = {loc_A: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_B: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_C: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_D: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_E: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_F: random.choice(['Clean', 'Dirty']),}\n",
    "\n",
    "    def thing_classes(self):\n",
    "        return [ReflexVacuumAgent]\n",
    "\n",
    "    def percept(self, agent):\n",
    "        \"\"\"Retorna la posición del agente y el estado de la posición (Dirty/Clean).\"\"\"\n",
    "        return (agent.location, self.status[agent.location])\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        \"\"\"Implementa el MAPA DE TRANSICIÓN: Cambia la posición del agente y/o el estado de la posición; \n",
    "        Cada aspiración (acción 'suck') en una localización Dirty provoca un aumento de desempeño en 10 unidades;\n",
    "        Cada movida efectiva Right, Left, Up o Down provoca una disminución de desempeño en 1 unidad \"\"\"\n",
    "        \n",
    "        if action == 'Suck':\n",
    "            self.status[agent.location] = 'Clean'\n",
    "            agent.performance += 10\n",
    "        else:\n",
    "            # Completar los movimientos para el agente\n",
    "            # Debes identificar la direccion de la accion y modificar acordemente el x o y\n",
    "            # Pista: Ten cuidado con los limites del ambiente!\n",
    "            actual_x, actual_y = agent.location\n",
    "\n",
    "            # Complete su codigo aqui\n",
    "            # Solución a la pregunta 1. El else último que añadí se puede quitar y solo penalizar cuando se mueve.\n",
    "            # Sin embargo, lo dejé de ahí asumiendo que los evaluadores no quieren que se modifique el código.\n",
    "            # Igual cumple con su cometido. La solución a la pregunta 3 con el código modificado la he colocado\n",
    "            # como un bloque separado junto con las preguntas de la sección inferior.\n",
    "            delta_x, delta_y = 0, 0\n",
    "            if action == 'Down':\n",
    "                delta_x = 1\n",
    "            elif action == 'Left':\n",
    "                delta_y = -1\n",
    "            elif action == 'Right':\n",
    "                delta_y = 1\n",
    "            elif action == 'Up':\n",
    "                delta_x = -1\n",
    "            next_x, next_y = actual_x + delta_x, actual_y + delta_y\n",
    "            # Si no nos hemos salido del tablero, realizamos un movimiento efectivo\n",
    "            if 0 <= next_x and next_x <= 1 and 0 <= next_y and next_y <= 2:\n",
    "                actual_x += delta_x\n",
    "                actual_y += delta_y\n",
    "            else:\n",
    "                # Si nos salimos del tablero, como no realizaremos un movimiento efectivo, le sumo uno para\n",
    "                # neutralizar la penalización.\n",
    "                agent.performance += 1\n",
    "            agent.location = actual_x, actual_y\n",
    "            agent.performance -= 1\n",
    "\n",
    "    def default_location(self, thing):\n",
    "        \"\"\"Devuelve una posicion aleatoria.\"\"\"\n",
    "        return random.choice([loc_A, loc_B, loc_C, loc_D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ETaNXGAoRDs"
   },
   "source": [
    "# Agente Aspirador de 6 posiciones con Programa Reactivo Simple\n",
    "\n",
    "Este agente es el agente aspirador de cuatro posiciones que usa un programa reactivo simple: realiza una accion basado en la percepción (posicion, estado) actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9pZHxLr3oSg_"
   },
   "outputs": [],
   "source": [
    "def ReflexVacuumAgent():\n",
    "    \n",
    "    def program(percept):\n",
    "        location, status = percept\n",
    "        if status == 'Dirty':\n",
    "            return 'Suck'\n",
    "        elif location == loc_A:\n",
    "            return 'Right'\n",
    "        elif location == loc_B:\n",
    "            return 'Right'\n",
    "        elif location == loc_C:\n",
    "            return 'Down'\n",
    "        elif location == loc_D:\n",
    "            return 'Up'\n",
    "        elif location == loc_E:\n",
    "            return 'Left'\n",
    "        elif location == loc_F:\n",
    "            return 'Left'\n",
    "    return Agent(program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV90QCzWpFeB"
   },
   "source": [
    "# Probando el agente reflexivo simple en su entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "B8_JDnAapG-U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado Inicial del Ambiente: {(0, 0): 'Clean', (0, 1): 'Dirty', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Clean'}\n",
      "ReflexVacuumAgent esta localizado en (0, 0) con desempeño = 0\n",
      "Estado del Ambiente despues de 6 pasos: {(0, 0): 'Clean', (0, 1): 'Clean', (0, 2): 'Clean', (1, 0): 'Dirty', (1, 1): 'Clean', (1, 2): 'Clean'}\n",
      "ReflexVacuumAgent esta localizado en (1, 0) con desempeño = 7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Crea el entorno del aspirador de 6 posiciones con 2 posiciones en estado 'Dirty'\"\"\"\n",
    "e = VacuumEnvironment()\n",
    "e.status = {loc_A: 'Clean',  loc_B: 'Dirty', loc_C: 'Clean', loc_D: 'Dirty', loc_E: 'Clean', loc_F: 'Clean'}\n",
    "\n",
    "\"\"\"Crea un agente reflexivo simple\"\"\"\n",
    "a = ReflexVacuumAgent()\n",
    "\n",
    "\"\"\"Añade el agente creado al entorno en posicion loc_A\"\"\"\n",
    "e.add_thing(a, location=loc_A) \n",
    "\n",
    "# Imprime el estado inicial del ambiente y localizacion del agente\n",
    "print(\"Estado Inicial del Ambiente: {}\".format(e.status))\n",
    "print(\"ReflexVacuumAgent esta localizado en {} con desempeño = {}\".format(a.location, a.performance))\n",
    "\n",
    "\"\"\"Ejecuta el entorno 6 pasos y obtiene el desempeño del agente\"\"\"\n",
    "e.run(6)\n",
    "\n",
    "# Imprime el estado actual del ambiente, localizacion del agente y su desempeño\n",
    "print(\"Estado del Ambiente despues de 6 pasos: {}\".format(e.status))\n",
    "print(\"ReflexVacuumAgent esta localizado en {} con desempeño = {}\".format(a.location, a.performance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdva6vJNpLFq"
   },
   "source": [
    "# Preguntas:\n",
    "1. Completar el codigo en la clase Vacuum Enviroment. No cambiar la medida de desempeño que tiene el ambiente, sólo completar la lógica de movimientos. (**4 pts**)\n",
    "\n",
    "Completado. Se obtuvo una medida de desempeño de 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Después de haber completado el código, ¿puede plantear un programa de Agente que maximice de mejor manera  la medida de desempeño implementada en el entorno?  No es necesario modificar el código, sino que puede explicarlo en sus propias palabras. (4 pts) (**4 pts**)\n",
    "\n",
    "Primero, en vez de penalizar el movimiento deberíamos penalizar la no limpieza al llegar a una casilla puesto que nuestra función quiere medir el éxito de limpiar. Luego, En ReflexVacuumAgent tenemos que se mueve en sentido horario, lo cual es una manera determinísitca de hacerlo. Realizar el movimiento de manera aleatoria sería mucho mejor ya que realmente la distribución de la suciedad en nuestro ambiente también estará distribuido de manera aleatoria. Para esto, en vez de manualmente con if's decidir el movimiento, podríamos realizar un random.choice. Se necesitan más matemáticas para probar que el valor esperado del número de movimientos cuando nos dirigimos en una dirección de manera aleatoria cuando el mapa está Sucio / Limpio inicialmente de manera aleatoria, debería ser menor que el valor esperado cuando nuestro movimiento está perfectamente determinado. Asimismo, en el entorno sería mejor poder modificar la percepción ya que, en un caso hipotético en el cual solo tengamos una suciedad en loc_D, en vez de dar toda la vuelta, si tan solo nos dirigiésemos hacia él y lo limpiamos, todo estaría acabado antes con un mejor desempéño en vez de tener que recorrer todo, en este tipo de casos, realizar una acción greedy nos ayudaría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Basado en el programa de agente original, ¿que debería cambiar en el entorno para que el programa de agente maximize la medida de desempeño? Modifique el código. (**4 pts**)\n",
    "\n",
    "Para maximizar la medida de desempeño, en vez de penalizar por el movimiento, deberíamos penalizar si es que la aspiradora limpió o no limpió en el lugar al cual ha llegado. Aquí está el código modificado, el cual corriendo el bloque de prueba obtuvo un desempeño de 7, superior al desempeño 5 de la pregunta 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Considere el siguiente ambiente:\n",
    "\n",
    "#############\n",
    "# A # B # C #\n",
    "#############\n",
    "# D # E # F #\n",
    "#############\n",
    "# Complete las posiciones reemplazando los guiones bajos por números\n",
    "loc_A, loc_B, loc_C, loc_D, loc_E, loc_F = (0,0), (0,1), (0, 2), (1, 0), (1, 1), (1, 2)\n",
    "\n",
    "class VacuumEnvironment(Environment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.status = {loc_A: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_B: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_C: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_D: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_E: random.choice(['Clean', 'Dirty']),\n",
    "                       loc_F: random.choice(['Clean', 'Dirty']),}\n",
    "\n",
    "    def thing_classes(self):\n",
    "        return [ReflexVacuumAgent]\n",
    "\n",
    "    def percept(self, agent):\n",
    "        \"\"\"Retorna la posición del agente y el estado de la posición (Dirty/Clean).\"\"\"\n",
    "        return (agent.location, self.status[agent.location])\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        \"\"\"Implementa el MAPA DE TRANSICIÓN: Cambia la posición del agente y/o el estado de la posición; \n",
    "        Cada aspiración (acción 'suck') en una localización Dirty provoca un aumento de desempeño en 10 unidades;\n",
    "        Cada movida efectiva Right, Left, Up o Down provoca una disminución de desempeño en 1 unidad \"\"\"\n",
    "        \n",
    "        if action == 'Suck':\n",
    "            self.status[agent.location] = 'Clean'\n",
    "            agent.performance += 10\n",
    "        else:\n",
    "            # Completar los movimientos para el agente\n",
    "            # Debes identificar la direccion de la accion y modificar acordemente el x o y\n",
    "            # Pista: Ten cuidado con los limites del ambiente!\n",
    "            actual_x, actual_y = agent.location\n",
    "\n",
    "            # Complete su codigo aqui\n",
    "            # Solución a la pregunta 3. Como queremos medir el desempeño de limpieza, lo que penalizaré será el hecho\n",
    "            # de que nuestro aspirador limpie o no el suelo.\n",
    "            delta_x, delta_y = 0, 0\n",
    "            if action == 'Down':\n",
    "                delta_x = 1\n",
    "            elif action == 'Left':\n",
    "                delta_y = -1\n",
    "            elif action == 'Right':\n",
    "                delta_y = 1\n",
    "            elif action == 'Up':\n",
    "                delta_x = -1\n",
    "            next_x, next_y = actual_x + delta_x, actual_y + delta_y\n",
    "            # Si no nos hemos salido del tablero, entonces el movimiento es válido\n",
    "            if 0 <= next_x and next_x <= 1 and 0 <= next_y and next_y <= 2:\n",
    "                actual_x += delta_x\n",
    "                actual_y += delta_y\n",
    "                agent.location = actual_x, actual_y\n",
    "                # Como la posicion del tablero es valida, entonces puedo verificar si es que está limpia o sucia\n",
    "                # Voy a penalizar si llegué a una posición limpia ya que no voy a realizar ninguna limpieza ahí\n",
    "                # en la siguiente acción.\n",
    "                if self.status[agent.location] == 'Clean':\n",
    "                    agent.performance -= 1\n",
    "\n",
    "    def default_location(self, thing):\n",
    "        \"\"\"Devuelve una posicion aleatoria.\"\"\"\n",
    "        return random.choice([loc_A, loc_B, loc_C, loc_D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Según el entorno, ¿existe la posibilidad de que el agente obtenga un desempeño sea negativo? En caso de que su respuesta sea positiva, explique como sería dicho escenario. (**4 pts**)\n",
    "\n",
    "Sí, existen muchas maneras de conseguirlo. Una manera sería que no le demos los pasos suficientes, por ejemplo, realizados n pasos, tenemos que nuestra función de desempeño sería f:N -> Z con f(n) = 10 * (# Sucks) - (# Pasos). En la distribución dada en el ejemplo tenemos\n",
    "\n",
    "Clean Dirty Clean\n",
    "\n",
    "Dirty Clean Clean\n",
    "\n",
    "En este caso particular notamos que si hacemos que ejecute el entorno tan solo 1 paso, nuestro desempeño sería -1.\n",
    "Asimismo, pasados los 23 pasos, nuestra función es negativa y decreciente puesto que todo ya está limpio (con el movimiento horario determinístico que se dio inicialmente).\n",
    "\n",
    "Finalmente, si el tablero hubiese estado completamente limpio\n",
    "\n",
    "Clean Clean Clean\n",
    "\n",
    "Clean Clean Clean\n",
    "\n",
    "hubiésemos tenido siempre resultados negativos con el programa original de la pregunta 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ¿Qué característica tendria que tener el entorno para necesitar un agente reactivo basado en modelo? (**4 pts**)\n",
    "\n",
    "Tendría que ser al menos un entorno parcialmente observable, es decir, depende del estado actual y no de toda la historia. En nuestro caso, la decisión se toma respecto a moverse en el mapa dependiendo de la posición, podríamos hacer que el estado de las casillas adyacentes a nosotros también influyan en nuestra toma de decisión, la cosa es que en nuestra toma de decisión no importó la historia sino como se encontraba el tablero a la hora de tomar una decisión entre limpiar o moverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Laboratorio1_2021-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
